# Pythonで機械学習をやる

### 基礎的な用語の説明
#### 活性化関数 Activation Function 
活性化関数は入力信号の総和がどのように活性化するか、つまり次の層に渡す値をどうするかを決める役割を持つ。
* 単純パーセプトロンの活性化関数ではステップ関数が使われる
* 多層パーセプトロンの活性化関数ではシグモイド関数、ソフトマックス関数、恒等関数が使われる

上記のステップ関数、シグモイド関数、ソフトマックス関数は非線形関数と呼ばれる。

一般的に多層パーセプトロンでは線形代数は使われない。

なぜなら活性化関数に線形関数を用いると層を深くすることの意味がなくなる。

どういうことかというとf(x) = cxを活性化関数として三層重ねると結局y = c**3xとして表せて、一層のネットワークでも表現できてしまう。
#### パーセプトロン　Perceptron
ある複数の入力に対して一個の出力を返す関数のこと。
多層パーセプトロンとはすなわち一般にイメージされるあれ
#### 閾値 Threshold
境を表す値。例えばでいうと痛みを表す値があったとして、ここの値を越えると痛みを感じると行った感じ。
#### ステップ関数 Step Function
ステップ関数は閾値を境にして出力が切り替わる関数のこと。
この場合ある入力より大きいとき1を返して、小さいとき0を返すと行った感じ。
#### シグモイド関数　Sigmoid Function
式で表すと

h(x) = 1 / ( 1 + np.exp(-x))

入力した値が大きければ大きいほど1に近づき、小さければ小さいほど0に近づく関数のこと。

ステップ関数の出力が0か1であることを考えれば、元の入力の値をより反映していると言えますね。
##### ReLU関数　ReLU Function
入力された値が0以下の時は0を出力し、1より大きい時は入力をそのまま出力する。
#### 恒等関数　Identity Function
出力層で使われる、入力に対してそのまま返す関数。
#### ソフトマックス関数　Softmax Function
出力層で用いられる、それぞれの入力を指数としてeを足した和。これをするから最終出力は0~1になる

### 学習モデルを構築する
学習モデルを構築するにはKerasが用いられる。
Kerasでは層を組み合わせてモデルを構築する。雰囲気濾過システムみたいなもん。
もっとも一般的なモデルは単純に層を積み重ねるtk.keras.Sequentialモデル(つまり全統合ネットワーク)。
```main.py
model = tf.keras.Sequential()
# ユニット数が64の全結合層をモデルに追加します：
model.add(layers.Dense(64, activation='relu'))
# 全結合層をもう一つ追加します：
model.add(layers.Dense(64, activation='relu'))
# 出力ユニット数が10のソフトマックス層を追加します：
model.add(layers.Dense(10, activation='softmax'))
```
tf.keras.layersは共通のコンストラクト引数がある
* activation:層の活性化関数を設定する。
ex)sigmoid relu
* kernel_initializer,bias_initializer:層の重み(カーネルとバイアス)の初期化方式
ex)kernel_initializer='orthogonal'#直交行列で初期化

* kernel_regularizer,bias_regularizer:層の重みに適用する正則化方式ex)tf.keras.regularizers.l1(0.01)#L1正則化を行う